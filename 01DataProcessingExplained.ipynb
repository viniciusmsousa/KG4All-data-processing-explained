{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Processing Explained\n",
    "\n",
    "This notebook was created in order to explain the data processing of the KG4All, that is the creation of a Knowledge Graph from natural language text. \n",
    "\n",
    "The notebok contains the followinf topics:\n",
    "\n",
    "- 1 Enviroment Setup (there are both a requirements.txt and a poetry.lock files)\n",
    "- 2 The data processing using the abstracts from the [COVID-19 Open Research Dataset Challenge (CORD-19)](https://www.kaggle.com/allen-institute-for-ai/CORD-19-research-challenge)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) Enviroment Setup\n",
    "\n",
    "To setup the enviroment it is needed to install the requirements.txt or poetry.lock file as well as the following command to instal the model. Note that this should be done just once ans it takes a while the install.\n",
    "\n",
    "``` !pip install https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_sm-0.2.4.tar.gz ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2) Data Processing\n",
    "\n",
    "In layman's terms the data processing receives natural language text as input and from the text extracts entities and related entities using the [ScispaCy Framework](https://arxiv.org/abs/1902.07669)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the libs and configuration the log file\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "import scispacy\n",
    "from scispacy.umls_linking import UmlsEntityLinker\n",
    "\n",
    "from loguru import logger\n",
    "logger.add(\"create_triplets_df.log\", rotation=\"50 MB\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Defining the scispacy class\n",
    "\n",
    "# Load entity extractor UMLS Model\n",
    "nlp = spacy.load(\"en_core_sci_sm\")\n",
    "\n",
    "# Load entity Linker\n",
    "linker = UmlsEntityLinker(resolve_abbreviations=True,max_entities_per_mention=3)\n",
    "\n",
    "# Merging Entity Extractor and Entity Linker\n",
    "nlp.add_pipe(linker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Loading the COVID-19 Open Research Dataset Challenge (CORD-19) abstracts dataset. YOU SHOULD DOWNLOAD THE metadata.csv FILE ANS SAVE IT IN THE SAME FOLDER AS THIS NOTEBOOK.\n",
    "\n",
    "df_abstracts = pd.read_csv(\"metadata.csv\").dropna(subset=['sha','abstract']).filter(['sha','abstract'])\n",
    "print(df_abstracts.shape)\n",
    "df_abstracts.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function definitions\n",
    "\n",
    "def get_text_entities(sha,text):\n",
    "    \"\"\"Extracts the UMLS entities from a text.\n",
    "\n",
    "    Args:\n",
    "        sha (str): Document id.\n",
    "        text (str): Natural Language text. \n",
    "\n",
    "    Returns:\n",
    "        Dict: Document id ans extracted entities.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        output={\n",
    "            'sha':sha,\n",
    "            'ents':[*nlp(text).ents]\n",
    "        }\n",
    "        return output \n",
    "    except Exception as e:\n",
    "        logger.error(f'Erro na funcao get_text_entities(): {e}')\n",
    "\n",
    "def get_ent_info_from_cui(cui):\n",
    "    \"\"\"Parser a bunch of information about the entity given its cui.\n",
    "\n",
    "    Args:\n",
    "        cui (str): entity cui as str.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Information about the entity of a goven cui.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        entity = linker.umls.cui_to_entity[cui]\n",
    "        name = entity[1]\n",
    "        alias = entity[2]\n",
    "        tui = entity[3]\n",
    "        semantic_descrition = entity[4]\n",
    "        \n",
    "        entity_json = {\n",
    "            'cui':cui,\n",
    "            'name':name,\n",
    "            'semantic_descrition':semantic_descrition\n",
    "        }\n",
    "        # In case entity is represented in by more than one tui\n",
    "        if len(tui) > 1:\n",
    "            entity_json['tui'] = [tui_i in tui]\n",
    "        else:\n",
    "            entity_json['tui'] = ','.join(tui)\n",
    "        return entity_json\n",
    "    except Exception as e:\n",
    "        logger.error(f'Erro na funcao get_ent_info_from_cui(): {e}')\n",
    "\n",
    "def create_head_json(sha,ent):\n",
    "    \"\"\"Creates a json for a head entiti extracted from a text.\n",
    "\n",
    "    Args:\n",
    "        sha (str): document id.\n",
    "        ent (scispacy ent): Anentity object from the scispacy package.\n",
    "\n",
    "    Returns:\n",
    "        Dict: Head entity information.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        head_cui = linker.umls.cui_to_entity[ent._.umls_ents[0][0]][0]\n",
    "        related_cui = [*map(lambda tuple_related:tuple_related[0],ent._.umls_ents)]\n",
    "        head_info_json = get_ent_info_from_cui(head_cui)\n",
    "        head_info_json['related_cui'] = related_cui\n",
    "        head_info_json['sha']=sha\n",
    "        return head_info_json\n",
    "    except Exception as e:\n",
    "        logger.error(f'Erro na funcao create_head_json(): {e}')\n",
    "\n",
    "def create_relation_df(head_entity):\n",
    "    \"\"\"Given a head entity it creates the the relations df.\n",
    "\n",
    "    Args:\n",
    "        head_entity (Dict): result from the create_head_json function.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: Data Frame with the relations\n",
    "    \"\"\"\n",
    "    try:\n",
    "        relations = []\n",
    "        for linked_cui in head_entity['related_cui']:\n",
    "            try:\n",
    "                linked_cui_info = get_ent_info_from_cui(linked_cui)\n",
    "                tail_tui = linked_cui_info['tui']\n",
    "                relation = {\n",
    "                        'head_sha':head_entity['sha'],\n",
    "                        'head_cui':head_entity['cui'],\n",
    "                        'head_name':head_entity['name'],\n",
    "                        'head_tui':head_entity['tui'],\n",
    "                        'head_semantic_descrition':head_entity['semantic_descrition'],\n",
    "                        'tail_cui':linked_cui_info['cui'],\n",
    "                        'tail_name':linked_cui_info['name'],\n",
    "                        'tail_semantic_descrition':linked_cui_info['semantic_descrition']\n",
    "                    }\n",
    "                if type(tail_tui) != str:\n",
    "                    relation['tail_tui'] = [*map(lambda x:x, tail_tui)]\n",
    "                else:\n",
    "                    relation['tail_tui'] = tail_tui\n",
    "                relations.append(relation)\n",
    "            except:\n",
    "                continue\n",
    "        return pd.DataFrame.from_dict(relations)\n",
    "    except Exception as e:\n",
    "        logger.error(f'Error na funfa create_relation_df(): {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# create the triplets dataframe\n",
    "\n",
    "n_abstracts=len(df_abstracts)\n",
    "triplets_file='2020-09-21-triplets.csv'\n",
    "for index,row in df_abstracts.iterrows():\n",
    "    try:\n",
    "        logger.debug(f'index: {index}')\n",
    "        sha=row['sha'][-40:],\n",
    "        text=row['abstract']\n",
    "        text_entities=get_text_entities(sha=sha,text=text)\n",
    "        head_entities = create_head_json(sha=text_entities['sha'],ent=text_entities['ents'][0])\n",
    "        df_relations = create_relation_df(head_entities)\n",
    "        # if file does not exist write header \n",
    "        if not os.path.isfile(triplets_file):\n",
    "            df_relations.to_csv(triplets_file, header='column_names',sep='|')\n",
    "        else: # else it exists so append without writing the header\n",
    "            df_relations.to_csv(triplets_file, mode='a', header=False,sep='|')\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8-final"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python37864bitmasterssandboxamnn0bmmpy37venv065205e102844f6899e3bc8e6a840507",
   "display_name": "Python 3.7.8 64-bit ('masters-sand-box-amnn0bMm-py3.7': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}